% Encoding: UTF-8

@inproceedings{Sethian1996,
  title={A fast marching level set method for monotonically advancing fronts},
  author={James~A. Sethian},
  booktitle={Proceedings of the National Academy of Sciences},
  year={1996}
}

@misc{schulman2017proximal,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{Wijmans2019DDPPOLN,
  title={{DD-PPO}: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames},
  author={Erik Wijmans and Abhishek Kadian and Ari S. Morcos and Stefan Lee and Irfan Essa and Devi Parikh and Manolis Savva and Dhruv Batra},
  booktitle={ICLR},
  year={2019}
} 
@misc{yarats2021mastering,
      title={Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning}, 
      author={Denis Yarats and Rob Fergus and Alessandro Lazaric and Lerrel Pinto},
      year={2021},
      eprint={2107.09645},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@INPROCEEDINGS{7780459,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}}


@inproceedings{Ramakrishnan2021HabitatMatterport3D,
  title={{Habitat-Matterport 3D Dataset} ({HM3D}): 1000 Large-scale {3D} Environments for Embodied {AI}},
  author={Santhosh K. Ramakrishnan and Aaron Gokaslan and Erik Wijmans and Oleksandr Maksymets and Alexander Clegg and John Turner and Eric Undersander and Wojciech Galuba and Andrew Westbury and Angel X. Chang and Manolis Savva and Yili Zhao and Dhruv Batra},
  booktitle={NeurIPS},
  year={2021},
}

@inproceedings{NEURIPS2021_021bbc7e,
 author = {Szot, Andrew and Clegg, Alexander and Undersander, Eric and Wijmans, Erik and Zhao, Yili and Turner, John and Maestre, Noah and Mukadam, Mustafa and Chaplot, Devendra Singh and Maksymets, Oleksandr and Gokaslan, Aaron and Vondru\v{s}, Vladim\'{\i}r and Dharur, Sameer and Meier, Franziska and Galuba, Wojciech and Chang, Angel and Kira, Zsolt and Koltun, Vladlen and Malik, Jitendra and Savva, Manolis and Batra, Dhruv},
 booktitle = {NeurIPS},
 title = {Habitat 2.0: Training Home Assistants to Rearrange their Habitat},
 year = {2021}
}


@article{Sutton2005ReinforcementLA,
  title={Reinforcement Learning: An Introduction},
  author={Richard S. Sutton and Andrew G. Barto},
  journal={IEEE Transactions on Neural Networks},
  year={2005},
  volume={16},
  pages={285-286}
}

@Article{ma16020683,
AUTHOR = {Siwek, Michał and Panasiuk, Jarosław and Baranowski, Leszek and Kaczmarek, Wojciech and Prusaczyk, Piotr and Borys, Szymon},
TITLE = {Identification of Differential Drive Robot Dynamic Model Parameters},
JOURNAL = {Materials},
VOLUME = {16},
YEAR = {2023},
NUMBER = {2},
ARTICLE-NUMBER = {683},
URL = {https://www.mdpi.com/1996-1944/16/2/683},
PubMedID = {36676421},
ISSN = {1996-1944},
}


@inproceedings{ros,
  title={{ROS}: an open-source Robot Operating System},
  author={Morgan Quigley and Brian Gerkey and Ken Conley and Josh Faust and Tully Foote and Jeremy Leibs and Eric Berger and Rob Wheeler and Andrew Ng},
  booktitle={ICRA, Workshop on Open Source Robotics},
  year={2009}
}

@misc{ros_documentation,
  title = {ROS Documentation},
  howpublished = {\url{http://docs.ros.org/}},
  author = {Open Robotics},
  year = {2023}
}

@misc{rosservices,
  title = {Understanding services},
  howpublished = {\url{https://docs.ros.org/en/humble/Tutorials/Beginner-CLI-Tools/Understanding-ROS2-Services/Understanding-ROS2-Services.html}},
  author = {Open Robotics},
  year = {2023}
}

@misc{rosdocker,
  title = {A Guide to Docker and ROS},
  howpublished = {\url{https://roboticseabass.com/2021/04/21/docker-and-ros/}},
  author = {Sebastián Castro},
  year = {2021}
}

@inproceedings{IntroductionTT,
  title={Introduction to the Time-of-Flight ( ToF ) System Design User ' s Guide},
  author={}
}

@software{orbeccros ,
  author = {Orbbec Ltd.},
  title = {{ROS wrapper for Astra camera}},
  url = {https://github.com/orbbec/ros_astra_camera},
  version = {1.2.2},
  year = {2023}
}

@software{kobuki ,
  author = {Kobuki Ltd.},
  title = {{ROS wrapper for Kobuki base Turtlebot 2}},
  url = {https://github.com/yujinrobot/kobuki.git},
  version = {melodic},
  year = {2023}
}

@inproceedings{andrychowicz2017,
  author    = {Marcin Andrychowicz and
               Dwight Crow and
               Alex Ray and
               Jonas Schneider and
               Rachel Fong and
               Peter Welinder and
               Bob McGrew and
               Josh Tobin and
               Pieter Abbeel and
               Wojciech Zaremba},
  title     = {Hindsight Experience Replay},
  booktitle = {NeurIPS},
  pages     = {5048--5058},
  year      = {2017},  
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100}
}

@misc{andrychowicz2020,
    title = {What {{Matters In On-Policy Reinforcement Learning}}? {{A Large-Scale Empirical Study}}},
    shorttitle = {What {{Matters In On-Policy Reinforcement Learning}}?},
    author = {Andrychowicz, Marcin and Raichuk, Anton and Sta{\'n}czyk, Piotr and Orsini, Manu and Girgin, Sertan and Marinier, Raphael and Hussenot, L{\'e}onard and Geist, Matthieu and Pietquin, Olivier and Michalski, Marcin and Gelly, Sylvain and Bachem, Olivier},
    year = {2020},
    month = jun,
    number = {arXiv:2006.05990},
    eprint = {2006.05990},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    publisher = {{arXiv}},
    doi = {10.48550/arXiv.2006.05990},    
    archiveprefix = {arXiv},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/7PI6L7ZB/Andrychowicz et al. - 2020 - What Matters In On-Policy Reinforcement Learning .pdf;/home/carlos/Zotero/storage/U8Q8YIES/2006.html}
}

@article{antoniou2019,
    title = {How to Train Your {{MAML}}},
    author = {Antoniou, Antreas and Edwards, Harrison and Storkey, Amos},
    year = {2019},
    month = mar,
    journal = {arXiv:1810.09502 [cs, stat]},
    eprint = {1810.09502},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    abstract = {The field of few-shot learning has recently seen substantial advancements. Most of these advancements came from casting few-shot learning as a meta-learning problem. Model Agnostic Meta Learning or MAML is currently one of the best approaches for few-shot learning via meta-learning. MAML is simple, elegant and very powerful, however, it has a variety of issues, such as being very sensitive to neural network architectures, often leading to instability during training, requiring arduous hyperparameter searches to stabilize training and achieve high generalization and being very computationally expensive at both training and inference times. In this paper, we propose various modifications to MAML that not only stabilize the system, but also substantially improve the generalization performance, convergence speed and computational overhead of MAML, which we call MAML++.},
    archiveprefix = {arXiv},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/RWGUQBQS/Antoniou et al. - 2019 - How to train your MAML.pdf;/home/carlos/Zotero/storage/5YY9R5LS/1810.html}
}

@inproceedings{badki2021,
    title = {Binary {{TTC}}: {{A}} Temporal Geofence for Autonomous Navigation},
    booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
    author = {Badki, Abhishek and Gallo, Orazio and Kautz, Jan and Sen, Pradeep},
    year = {2021},
    month = jun,
    pages = {12946--12955},
    keywords = {�� Skimmed},
    annotation = {00003},
    file = {/home/carlos/Zotero/storage/D3MWBSD5/Badki et al. - 2021 - Binary TTC A temporal geofence for autonomous nav.pdf}
}

@inproceedings{baik2021,
    title = {Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning},
    booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
    author = {Baik, Sungyong and Choi, Janghoon and Kim, Heewon and Cho, Dohee and Min, Jaesik and Lee, Kyoung Mu},
    year = {2021},
    month = oct,
    pages = {9465--9474},
    doi = {https://openaccess.thecvf.com/content/ICCV2021/supplemental/Baik_Meta-Learning_With_Task-Adaptive_ICCV_2021_supplemental.pdf},
    keywords = {✅ Read},
    annotation = {00000},
    file = {/home/carlos/Zotero/storage/6J8CJQ4R/Baik et al. - 2021 - Meta-learning with task-adaptive loss function for.pdf}
}

@inproceedings{batra2020,
    title = {{{ObjectNav Revisited}}: {{On Evaluation}} of {{Embodied Agents Navigating}} to {{Objects}}},
    booktitle = {{{arXiv}}},
    author = {Batra, Dhruv and Gokaslan, Aaron and Kembhavi, Aniruddha and Maksymets, Oleksandr and Mottaghi, Roozbeh and Savva, Manolis and Toshev, Alexander and Wijmans, Erik},
    year = {2020},
    eprint = {2006.13171},
    eprinttype = {arxiv},
    doi = {https://arxiv.org/abs/2006.13171},
    archiveprefix = {arXiv},
    keywords = {�� Skimmed},
    file = {/home/carlos/Zotero/storage/BSFGXJ67/Batra et al. - 2020 - ObjectNav Revisited On Evaluation of Embodied Age.pdf}
}

@inproceedings{beaulieu2020,
    title = {Learning to Continually Learn},
    booktitle = {{{ECAI}} 2020 - 24th European Conference on Artificial Intelligence, 29 August-8 September 2020, Santiago de Compostela, Spain, August 29 - September 8, 2020 - Including 10th Conference on Prestigious Applications of Artificial Intelligence ({{PAIS}} 2020)},
    author = {Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff and Cheney, Nick},
    editor = {Giacomo, Giuseppe De and Catal{\'a}, Alejandro and Dilkina, Bistra and Milano, Michela and Barro, Sen{\'e}n and Bugar{\'i}n, Alberto and Lang, J{\'e}r{\^o}me},
    year = {2020},
    series = {Frontiers in Artificial Intelligence and Applications},
    volume = {325},
    pages = {992--1001},
    publisher = {{IOS Press}},
    doi = {10.3233/FAIA200193},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/conf/ecai/BeaulieuFMLSCC20.bib},
    keywords = {❌ Unread},
    annotation = {00069},
    timestamp = {Fri, 09 Apr 2021 18:50:05 +0200},
    file = {/home/carlos/Zotero/storage/X25GYC82/Beaulieu et al. - 2020 - Learning to continually learn.pdf}
}

@article{berseth2021,
    title = {{{CoMPS}}: {{Continual Meta Policy Search}}},
    shorttitle = {{{CoMPS}}},
    author = {Berseth, Glen and Zhang, Zhiwei and Zhang, Grace and Finn, Chelsea and Levine, Sergey},
    year = {2021},
    month = dec,
    journal = {arXiv:2112.04467 [cs]},
    eprint = {2112.04467},
    eprinttype = {arxiv},
    primaryclass = {cs},
    abstract = {We develop a new continual meta-learning method to address challenges in sequential multi-task learning. In this setting, the agent's goal is to achieve high reward over any sequence of tasks quickly. Prior meta-reinforcement learning algorithms have demonstrated promising results in accelerating the acquisition of new tasks. However, they require access to all tasks during training. Beyond simply transferring past experience to new tasks, our goal is to devise continual reinforcement learning algorithms that learn to learn, using their experience on previous tasks to learn new tasks more quickly. We introduce a new method, continual meta-policy search (CoMPS), that removes this limitation by meta-training in an incremental fashion, over each task in a sequence, without revisiting prior tasks. CoMPS continuously repeats two subroutines: learning a new task using RL and using the experience from RL to perform completely offline meta-learning to prepare for subsequent task learning. We find that CoMPS outperforms prior continual learning and off-policy meta-reinforcement methods on several sequences of challenging continuous control tasks.},
    archiveprefix = {arXiv},
    keywords = {�� Skimmed},
    file = {/home/carlos/Zotero/storage/Z5FU7U65/Berseth et al. - 2021 - CoMPS Continual Meta Policy Search.pdf;/home/carlos/Zotero/storage/AQA38ZLN/2112.html}
}

@article{chang2017,
    title = {{{Matterport3D}}: {{Learning}} from {{RGB-D Data}} in {{Indoor Environments}}},
    shorttitle = {{{Matterport3D}}},
    author = {Chang, Angel and Dai, Angela and Funkhouser, Thomas and Halber, Maciej and Nie{\ss}ner, Matthias and Savva, Manolis and Song, Shuran and Zeng, Andy and Zhang, Yinda},
    year = {2017},
    month = sep,
    journal = {arXiv:1709.06158 [cs]},
    eprint = {1709.06158},
    eprinttype = {arxiv},
    primaryclass = {cs},
    abstract = {Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.},
    archiveprefix = {arXiv},
    keywords = {✅ Read},
    file = {/home/carlos/Zotero/storage/PI2Z9LCF/Chang et al. - 2017 - Matterport3D Learning from RGB-D Data in Indoor E.pdf;/home/carlos/Zotero/storage/5K7765Z5/1709.html}
}

@inproceedings{chang2020,
    title = {Semantic {{Visual Navigation}} by {{Watching Youtube Videos}}},
    booktitle = {{{NeurIPS}}},
    author = {Chang, Matthew and Gupta, Arjun and Gupta, Saurabh},
    year = {2020},
    keywords = {✅ Read},
    annotation = {00017},
    file = {/home/carlos/Zotero/storage/P7YFGYMM/Chang2020.pdf;/Users/carlos/Documents/Trabajo/Doctorado/Presentaciones/Reuniones Roberto/Chang2020.key}
}

@inproceedings{chaplot2020,
    title = {Object {{Goal Navigation}} Using {{Goal-Oriented Semantic Exploration}}},
    author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
    year = {2020},
    booktitle = {NeurIPS},
}

@inproceedings{chen2021,
    title = {Meta-Baseline: {{Exploring}} Simple Meta-Learning for Few-Shot Learning},
    booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
    author = {Chen, Yinbo and Liu, Zhuang and Xu, Huijuan and Darrell, Trevor and Wang, Xiaolong},
    year = {2021},
    month = oct,
    pages = {9062--9071},
    doi = {https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Meta-Baseline_Exploring_Simple_Meta-Learning_for_Few-Shot_Learning_ICCV_2021_paper.pdf},
    keywords = {�� Skimmed},
    annotation = {00002},
    file = {/home/carlos/Zotero/storage/QJ8CUXFP/Chen et al. - 2021 - Meta-baseline Exploring simple meta-learning for .pdf}
}

@inproceedings{chen2021a,
    title = {Topological Planning with Transformers for Vision-and-Language Navigation},
    booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
    author = {Chen, Kevin and Chen, Junshen K. and Chuang, Jo and Vazquez, Marynel and Savarese, Silvio},
    year = {2021},
    month = jun,
    pages = {11276--11286},
    keywords = {�� Skimmed},
    annotation = {00008},
    file = {/home/carlos/Zotero/storage/XRC3B4WP/Chen et al. - 2021 - Topological planning with transformers for vision-.pdf}
}

@inproceedings{chen2021b,
    title = {Learning Generalizable Robotic Reward Functions from ``{{In-The-Wild}}'' Human Videos},
    booktitle = {Proceedings of Robotics: {{Science}} and Systems},
    author = {Chen, Annie S. and Nair, Suraj and Finn, Chelsea},
    year = {2021},
    month = jul,
    address = {{Virtual}},
    doi = {10.15607/RSS.2021.XVII.012},
    keywords = {❌ Unread},
    annotation = {00005},
    file = {/home/carlos/Zotero/storage/65KJLLQL/Chen et al. - 2021 - Learning generalizable robotic reward functions fr.pdf}
}

@misc{deitke2022,
    title = {{{ProcTHOR}}: {{Large-Scale Embodied AI Using Procedural Generation}}},
    shorttitle = {{{ProcTHOR}}},
    author = {Deitke, Matt and VanderBilt, Eli and Herrasti, Alvaro and Weihs, Luca and Salvador, Jordi and Ehsani, Kiana and Han, Winson and Kolve, Eric and Farhadi, Ali and Kembhavi, Aniruddha and Mottaghi, Roozbeh},
    year = {2022},
    month = jun,
    number = {arXiv:2206.06994},
    eprint = {2206.06994},
    eprinttype = {arxiv},
    primaryclass = {cs},
    publisher = {{arXiv}},
    doi = {10.48550/arXiv.2206.06994},
    abstract = {Massive datasets and high-capacity models have driven many recent advancements in computer vision and natural language understanding. This work presents a platform to enable similar success stories in Embodied AI. We propose ProcTHOR, a framework for procedural generation of Embodied AI environments. ProcTHOR enables us to sample arbitrarily large datasets of diverse, interactive, customizable, and performant virtual environments to train and evaluate embodied agents across navigation, interaction, and manipulation tasks. We demonstrate the power and potential of ProcTHOR via a sample of 10,000 generated houses and a simple neural model. Models trained using only RGB images on ProcTHOR, with no explicit mapping and no human task supervision produce state-of-the-art results across 6 embodied AI benchmarks for navigation, rearrangement, and arm manipulation, including the presently running Habitat 2022, AI2-THOR Rearrangement 2022, and RoboTHOR challenges. We also demonstrate strong 0-shot results on these benchmarks, via pre-training on ProcTHOR with no fine-tuning on the downstream benchmark, often beating previous state-of-the-art systems that access the downstream training data.},
    archiveprefix = {arXiv},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/3YHNCBT6/Deitke et al. - 2022 - ProcTHOR Large-Scale Embodied AI Using Procedural.pdf;/home/carlos/Zotero/storage/LFMQA85X/2206.html}
}

@article{duan2016,
    title = {{{RL}}\$\^2\$: {{Fast Reinforcement Learning}} via {{Slow Reinforcement Learning}}},
    shorttitle = {{{RL}}\$\^2\$},
    author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
    year = {2016},
    month = nov,
    journal = {arXiv:1611.02779 [cs, stat]},
    eprint = {1611.02779},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL\$\^2\$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL\$\^2\$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL\$\^2\$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL\$\^2\$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
    archiveprefix = {arXiv},
    keywords = {�� Skimmed},
    file = {/home/carlos/Zotero/storage/5VU5S8CA/Duan et al. - 2016 - RL$^2$ Fast Reinforcement Learning via Slow Reinf.pdf;/home/carlos/Zotero/storage/65W6AXMI/1611.html}
}

@inproceedings{fakoor2020,
    title = {Meta-q-Learning},
    booktitle = {International Conference on Learning Representations},
    author = {Fakoor, Rasool and Chaudhari, Pratik and Soatto, Stefano and Smola, Alexander J.},
    year = {2020},
    doi = {https://openreview.net/pdf?id=SJeD3CEFPH},
    keywords = {�� To-Read},
    annotation = {00050},
    file = {/home/carlos/Zotero/storage/WHKX265U/Fakoor et al. - 2020 - Meta-q-learning.pdf}
}

@misc{fallah2021,
    title = {On the {{Convergence Theory}} of {{Debiased Model-Agnostic Meta-Reinforcement Learning}}},
    author = {Fallah, Alireza and Georgiev, Kristian and Mokhtari, Aryan and Ozdaglar, Asuman},
    year = {2021},
    month = nov,
    number = {arXiv:2002.05135},
    eprint = {2002.05135},
    eprinttype = {arxiv},
    primaryclass = {cs, math, stat},
    publisher = {{arXiv}},
    abstract = {We consider Model-Agnostic Meta-Learning (MAML) methods for Reinforcement Learning (RL) problems, where the goal is to find a policy using data from several tasks represented by Markov Decision Processes (MDPs) that can be updated by one step of stochastic policy gradient for the realized MDP. In particular, using stochastic gradients in MAML update steps is crucial for RL problems since computation of exact gradients requires access to a large number of possible trajectories. For this formulation, we propose a variant of the MAML method, named Stochastic Gradient Meta-Reinforcement Learning (SG-MRL), and study its convergence properties. We derive the iteration and sample complexity of SG-MRL to find an \$\textbackslash epsilon\$-first-order stationary point, which, to the best of our knowledge, provides the first convergence guarantee for model-agnostic meta-reinforcement learning algorithms. We further show how our results extend to the case where more than one step of stochastic policy gradient method is used at test time. Finally, we empirically compare SG-MRL and MAML in several deep RL environments.},
    archiveprefix = {arXiv},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/I7VL9RPT/Fallah et al. - 2021 - On the Convergence Theory of Debiased Model-Agnost.pdf;/home/carlos/Zotero/storage/IA5N6FLU/2002.html}
}

@inproceedings{finn2017,
    title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
    booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
    author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
    editor = {Precup, Doina and Teh, Yee Whye},
    year = {2017},
    month = aug,
    series = {Proceedings of {{Machine Learning Research}}},
    volume = {70},
    pages = {1126--1135},
    publisher = {{PMLR}},
    abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
    keywords = {✅ Read},
    annotation = {04843},
    file = {/home/carlos/Zotero/storage/PH5EQQ7P/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf}
}

@inproceedings{fujimoto2018,
    title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
    booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
    author = {Fujimoto, Scott and Hoof, Herke and Meger, David},
    year = {2018},
    month = jul,
    pages = {1587--1596},
    publisher = {{PMLR}},
    issn = {2640-3498},
    abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
    langid = {english},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/4S4YQG36/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf;/home/carlos/Zotero/storage/LCTIR6MI/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf}
}

@article{ghadirzadeh2022,
    title = {Training and {{Evaluation}} of {{Deep Policies}} Using {{Reinforcement Learning}} and {{Generative Models}}},
    author = {Ghadirzadeh, Ali and Poklukar, Petra and Arndt, Karol and Finn, Chelsea and Kyrki, Ville and Kragic, Danica and Bj{\"o}rkman, M{\aa}rten},
    year = {2022},
    month = apr,
    journal = {arXiv:2204.08573 [cs]},
    eprint = {2204.08573},
    eprinttype = {arxiv},
    primaryclass = {cs},
    abstract = {We present a data-efficient framework for solving sequential decision-making problems which exploits the combination of reinforcement learning (RL) and latent variable generative models. The framework, called GenRL, trains deep policies by introducing an action latent variable such that the feed-forward policy search can be divided into two parts: (i) training a sub-policy that outputs a distribution over the action latent variable given a state of the system, and (ii) unsupervised training of a generative model that outputs a sequence of motor actions conditioned on the latent action variable. GenRL enables safe exploration and alleviates the data-inefficiency problem as it exploits prior knowledge about valid sequences of motor actions. Moreover, we provide a set of measures for evaluation of generative models such that we are able to predict the performance of the RL policy training prior to the actual training on a physical robot. We experimentally determine the characteristics of generative models that have most influence on the performance of the final policy training on two robotics tasks: shooting a hockey puck and throwing a basketball. Furthermore, we empirically demonstrate that GenRL is the only method which can safely and efficiently solve the robotics tasks compared to two state-of-the-art RL methods.},
    archiveprefix = {arXiv},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/K3UECBNS/Ghadirzadeh et al. - 2022 - Training and Evaluation of Deep Policies using Rei.pdf;/home/carlos/Zotero/storage/4CPXDT6S/2204.html}
}

@book{goodfellow2016,
    title = {Deep {{Learning}}},
    author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
    year = {2016},
    publisher = {{MIT Press}},
    annotation = {00000}
}

@article{hospedales2020,
    title = {Meta-Learning in Neural Networks: {{A}} Survey},
    author = {Hospedales, Timothy M. and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos J.},
    year = {2020},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    eprint = {2004.05439},
    eprinttype = {arxiv},
    doi = {10.1109/TPAMI.2021.3079209},
    archiveprefix = {arXiv},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/journals/corr/abs-2004-05439.bib},
    keywords = {�� To-Read},
    annotation = {00340},
    timestamp = {Tue, 14 Apr 2020 16:40:34 +0200; https://web.archive.org/web/20211129214707/https://arxiv.org/abs/2004.05439},
    file = {/home/carlos/Zotero/storage/LW629GDY/Hospedales et al. - 2020 - Meta-learning in neural networks A survey.pdf}
}

@inproceedings{ishida2022,
    title = {Towards {{Real-World Navigation With Deep Differentiable Planners}}},
    booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
    author = {Ishida, Shu and Henriques, Jo{\~a}o F.},
    year = {2022},
    pages = {17327--17336},
    langid = {english},
    keywords = {�� To-Read},
    file = {/home/carlos/Zotero/storage/EDJNX2G7/CVPR2022_CALVIN_supplementary.pdf;/home/carlos/Zotero/storage/UX3KW95K/Ishida and Henriques - 2022 - Towards Real-World Navigation With Deep Differenti.pdf;/home/carlos/Zotero/storage/2K64LRDQ/Ishida_Towards_Real-World_Navigation_With_Deep_Differentiable_Planners_CVPR_2022_paper.html}
}

@inproceedings{jabri2019,
 author = {Jabri, Allan and Hsu, Kyle and Gupta, Abhishek and Eysenbach, Ben and Levine, Sergey and Finn, Chelsea},
 booktitle = {NeurIPS},
 pages = {},
 title = {Unsupervised Curricula for Visual Meta-Reinforcement Learning},
 volume = {32},
 year = {2019}
}


@inproceedings{jaderberg2016,
  author    = {Max Jaderberg and
               Volodymyr Mnih and
               Wojciech Marian Czarnecki and
               Tom Schaul and
               Joel Z. Leibo and
               David Silver and
               Koray Kavukcuoglu},
  title     = {Reinforcement Learning with Unsupervised Auxiliary Tasks},
  booktitle = {ICLR},
  year      = {2017},
  timestamp = {Thu, 14 Oct 2021 10:00:37 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/JaderbergMCSLSK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{javed2019,
    title = {Meta-{{Learning Representations}} for {{Continual Learning}}},
    booktitle = {Advances in {{Neural Information Processing Systems}}},
    author = {Javed, Khurram and White, Martha},
    year = {2019},
    volume = {32},
    publisher = {{Curran Associates, Inc.}},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/67T82LID/Javed y White - 2019 - Meta-Learning Representations for Continual Learni.pdf}
}

@article{kadian2020,
    title = {{{Sim2Real}} Predictivity: {{Does}} Evaluation in Simulation Predict Real-World Performance?},
    author = {Kadian, Abhishek and Truong, Joanne and Gokaslan, Aaron and Clegg, Alexander and Wijmans, Erik and Lee, Stefan and Savva, Manolis and Chernova, Sonia and Batra, Dhruv},
    year = {2020},
    journal = {IEEE Robotics and Automation Letters},
    file = {/home/carlos/Zotero/storage/78DBWX4S/Kadian et al. - 2020 - Sim2Real predictivity Does evaluation in simulati.pdf}
}

@misc{kahn2020,
    title = {{{BADGR}}: {{An Autonomous Self-Supervised Learning-Based Navigation System}}},
    shorttitle = {{{BADGR}}},
    author = {Kahn, Gregory and Abbeel, Pieter and Levine, Sergey},
    year = {2020},
    month = apr,
    number = {arXiv:2002.05700},
    eprint = {2002.05700},
    eprinttype = {arxiv},
    primaryclass = {cs},
    institution = {{arXiv}},
    abstract = {Mobile robot navigation is typically regarded as a geometric problem, in which the robot's objective is to perceive the geometry of the environment in order to plan collision-free paths towards a desired goal. However, a purely geometric view of the world can can be insufficient for many navigation problems. For example, a robot navigating based on geometry may avoid a field of tall grass because it believes it is untraversable, and will therefore fail to reach its desired goal. In this work, we investigate how to move beyond these purely geometric-based approaches using a method that learns about physical navigational affordances from experience. Our approach, which we call BADGR, is an end-to-end learning-based mobile robot navigation system that can be trained with self-supervised off-policy data gathered in real-world environments, without any simulation or human supervision. BADGR can navigate in real-world urban and off-road environments with geometrically distracting obstacles. It can also incorporate terrain preferences, generalize to novel environments, and continue to improve autonomously by gathering more data. Videos, code, and other supplemental material are available on our website https://sites.google.com/view/badgr},
    archiveprefix = {arXiv},
    keywords = {�� To-Read},
    file = {/home/carlos/Zotero/storage/J2QC5GWH/Kahn et al. - 2020 - BADGR An Autonomous Self-Supervised Learning-Base.pdf;/home/carlos/Zotero/storage/D7MAXQYP/2002.html}
}

@inproceedings{karkus2021,
    title = {Differentiable {{SLAM-Net}}: {{Learning}} Particle {{SLAM}} for Visual Navigation},
    booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
    author = {Karkus, Peter and Cai, Shaojun and Hsu, David},
    year = {2021},
    month = jun,
    pages = {2815--2825},
    keywords = {❌ Unread},
    annotation = {00005},
    file = {/home/carlos/Zotero/storage/SRL96VGX/Karkus et al. - 2021 - Differentiable SLAM-Net Learning particle SLAM fo.pdf}
}

@inproceedings{kaushik2020,
    title = {Fast Online Adaptation in Robotics through Meta-Learning Embeddings of Simulated Priors},
    booktitle = {2020 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems ({{IROS}})},
    author = {Kaushik, Rituraj and Anne, Timoth{\'e}e and Mouret, Jean-Baptiste},
    year = {2020},
    pages = {5269--5276},
    doi = {10.1109/IROS45743.2020.9341462},
    keywords = {�� Skimmed},
    annotation = {00010},
    file = {/home/carlos/Zotero/storage/SCMWF7JP/Kaushik et al. - 2020 - Fast online adaptation in robotics through meta-le.pdf}
}

@misc{kempka2016,
    title = {{{ViZDoom}}: {{A Doom-based AI Research Platform}} for {{Visual Reinforcement Learning}}},
    shorttitle = {{{ViZDoom}}},
    author = {Kempka, Micha{\l} and Wydmuch, Marek and Runc, Grzegorz and Toczek, Jakub and Ja{\'s}kowski, Wojciech},
    year = {2016},
    month = sep,
    number = {arXiv:1605.02097},
    eprint = {1605.02097},
    eprinttype = {arxiv},
    primaryclass = {cs},
    institution = {{arXiv}},
    abstract = {The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.},
    archiveprefix = {arXiv},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/AK2JUZD2/Kempka et al. - 2016 - ViZDoom A Doom-based AI Research Platform for Vis.pdf;/home/carlos/Zotero/storage/KDJMB6Y8/1605.html}
}

@inproceedings{khandelwal2022,
    title = {Simple but {{Effective}}: {{CLIP Embeddings}} for {{Embodied AI}}},
    author = {Khandelwal, Apoorv and Weihs, Luca and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
    booktitle = {CVPR},
    year = {2022},
    abstract = {Contrastive language image pretraining (CLIP) encoders have been shown to be beneficial for a range of visual tasks from classification and detection to captioning and image manipulation. We investigate the effectiveness of CLIP visual backbones for Embodied AI tasks. We build incredibly simple baselines, named EmbCLIP, with no task specific architectures, inductive biases (such as the use of semantic maps), auxiliary tasks during training, or depth maps\textemdash yet we find that our improved baselines perform very well across a range of tasks and simulators. EmbCLIP tops the RoboTHOR ObjectNav leaderboard by a huge margin of 20 pts (Success Rate). It tops the iTHOR 1-Phase Rearrangement leaderboard, beating the next best submission, which employs Active Neural Mapping, and more than doubling the \% Fixed Strict metric (0.08 to 0.17). It also beats the winners of the 2021 Habitat ObjectNav Challenge, which employ auxiliary tasks, depth maps, and human demonstrations, and those of the 2019 Habitat PointNav Challenge. We evaluate the ability of CLIP's visual representations at capturing semantic information about input observations\textemdash primitives that are useful for navigation-heavy embodied tasks\textemdash and find that CLIP's representations encode these primitives more effectively than ImageNet-pretrained backbones. Finally, we extend one of our baselines, producing an agent capable of zero-shot object navigation that can navigate to objects that were not used as targets during training. Our code and models are available at https://github.com/ allenai/embodied-clip.},
    langid = {english},
    keywords = {✅ Read},
    file = {/home/carlos/Zotero/storage/IRA2A83B/Khandelwal et al. - Simple but Effective CLIP Embeddings for Embodied.pdf}
}

@inproceedings{koh2021,
    title = {Pathdreamer: {{A}} World Model for Indoor Navigation},
    booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision ({{ICCV}})},
    author = {Koh, Jing Yu and Lee, Honglak and Yang, Yinfei and Baldridge, Jason and Anderson, Peter},
    year = {2021},
    month = oct,
    pages = {14738--14748},
    doi = {arxiv:2105.08756},
    keywords = {�� Skimmed},
    annotation = {00000},
    file = {/home/carlos/Zotero/storage/BKTVRZLV/Koh et al. - 2021 - Pathdreamer A world model for indoor navigation.pdf}
}

@article{levine2020,
    title = {Offline {{Reinforcement Learning}}: {{Tutorial}}, {{Review}}, and {{Perspectives}} on {{Open Problems}}},
    shorttitle = {Offline {{Reinforcement Learning}}},
    author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
    year = {2020},
    month = nov,
    journal = {arXiv:2005.01643 [cs, stat]},
    eprint = {2005.01643},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
    archiveprefix = {arXiv},
    keywords = {�� To-Read},
    file = {/home/carlos/Zotero/storage/QIMM2SWW/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, .pdf;/home/carlos/Zotero/storage/SK7CKGCX/2005.html}
}

@article{li2018,
    title = {Learning to {{Generalize}}: {{Meta-Learning}} for {{Domain Generalization}}},
    shorttitle = {Learning to {{Generalize}}},
    author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy},
    year = {2018},
    month = apr,
    journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
    volume = {32},
    number = {1},
    issn = {2374-3468},
    doi = {10.1609/aaai.v32i1.11596},
    abstract = {Domain shift refers to the well known problem that a model trained in one source domain performs poorly when appliedto a target domain with different statistics. Domain Generalization (DG) techniques attempt to alleviate this issue by producing models which by design generalize well to novel testing domains. We propose a novel meta-learning method for domain generalization. Rather than designing a specific model that is robust to domain shift as in most previous DG work, we propose a model agnostic training procedure for DG. Our algorithm simulates train/test domain shift during training by synthesizing virtual testing domains within each mini-batch. The meta-optimization objective requires that steps to improve training domain performance should also improve testing domain performance. This meta-learning procedure trains models with good generalization ability to novel domains. We evaluate our method and achieve state of the art results on a recent cross-domain image classification benchmark, as well demonstrating its potential on two classic reinforcement learning tasks.},
    copyright = {Copyright (c)},
    langid = {english},
    keywords = {�� Skimmed},
    file = {/home/carlos/Zotero/storage/N9I63A8S/Li et al. - 2018 - Learning to Generalize Meta-Learning for Domain G.pdf}
}

@inproceedings{li2020,
    title = {Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation},
    booktitle = {CVPR},
    author = {Li, Juncheng and Wang, Xin and Tang, Siliang and Shi, Haizhou and Wu, Fei and Zhuang, Yueting and Wang, William Yang},
    year = {2020},
    month = jun,
    keywords = {�� To-Read},
    annotation = {00023},
    file = {/home/carlos/Zotero/storage/KWWVXLFL/Li et al. - 2020 - Unsupervised reinforcement learning of transferabl.pdf}
}

@misc{liu2021,
    title = {Decoupling {{Exploration}} and {{Exploitation}} for {{Meta-Reinforcement Learning}} without {{Sacrifices}}},
    author = {Liu, Evan Zheran and Raghunathan, Aditi and Liang, Percy and Finn, Chelsea},
    year = {2021},
    month = nov,
    number = {arXiv:2008.02790},
    eprint = {2008.02790},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    institution = {{arXiv}},
    abstract = {The goal of meta-reinforcement learning (meta-RL) is to build agents that can quickly learn new tasks by leveraging prior experience on related tasks. Learning a new task often requires both exploring to gather task-relevant information and exploiting this information to solve the task. In principle, optimal exploration and exploitation can be learned end-to-end by simply maximizing task performance. However, such meta-RL approaches struggle with local optima due to a chicken-and-egg problem: learning to explore requires good exploitation to gauge the exploration's utility, but learning to exploit requires information gathered via exploration. Optimizing separate objectives for exploration and exploitation can avoid this problem, but prior meta-RL exploration objectives yield suboptimal policies that gather information irrelevant to the task. We alleviate both concerns by constructing an exploitation objective that automatically identifies task-relevant information and an exploration objective to recover only this information. This avoids local optima in end-to-end training, without sacrificing optimal exploration. Empirically, DREAM substantially outperforms existing approaches on complex meta-RL problems, such as sparse-reward 3D visual navigation. Videos of DREAM: https://ezliu.github.io/dream/},
    archiveprefix = {arXiv},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/DUMH3KI3/Liu et al. - 2021 - Decoupling Exploration and Exploitation for Meta-R.pdf;/home/carlos/Zotero/storage/IB72YPHI/2008.html}
}

@inproceedings{lunagutierrez2020,
    title = {Information-Theoretic {{Task Selection}} for {{Meta-Reinforcement Learning}}},
    booktitle = {Advances in {{Neural Information Processing Systems}}},
    author = {Luna Gutierrez, Ricardo and Leonetti, Matteo},
    year = {2020},
    volume = {33},
    pages = {20532--20542},
    publisher = {{Curran Associates, Inc.}},
    abstract = {In Meta-Reinforcement Learning (meta-RL) an agent is trained on a set of tasks to prepare for and learn faster in new, unseen, but related tasks. The training tasks are usually hand-crafted to be representative of the expected distribution of target tasks and hence all used in training. We show that given a set of training tasks, learning can be both faster and more effective (leading to better performance in the target tasks), if the training tasks are appropriately selected. We propose a task selection algorithm based on information theory, which optimizes the set of tasks used for training in meta-RL, irrespectively of how they are generated. The algorithm establishes which training tasks are both sufficiently relevant for the target tasks, and different enough from one another. We reproduce different meta-RL experiments from the literature and show that our task selection algorithm improves the final performance in all of them.},
    keywords = {�� To-Read},
    file = {/home/carlos/Zotero/storage/9MQTFCGH/SuplementaryMaterial.pdf;/home/carlos/Zotero/storage/HZMQKC3N/Luna Gutierrez and Leonetti - 2020 - Information-theoretic Task Selection for Meta-Rein.pdf}
}

@inproceedings{luo2021,
    title = {A Few Shot Adaptation of Visual Navigation Skills to New Observations Using Meta-Learning},
    booktitle = {{ICRA}},
    author = {Luo, Qian and Sorokin, Maks and Ha, Sehoon},
    year = {2021},
    pages = {13231--13237},
    keywords = {✅ Read},
    annotation = {00002},
    file = {/home/carlos/Zotero/storage/X5DESP8I/A_Few_Shot_Adaptation_of_Visual_Navigation_Skills_to_New_Observations_using_Meta-Learning.pdf;/Users/carlos/Documents/Trabajo/Doctorado/Presentaciones/Seminarios/Luo2021.key}
}

@article{macenski2020,
    title = {The Marathon 2: {{A}} Navigation System},
    author = {Macenski, Steve and Martin, Francisco and White, Ruffin and Clavero, Jonatan Gines},
    year = {2020},
    month = oct,
    journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
    publisher = {{IEEE; https://web.archive.org/web/20211124165243/https://ieeexplore.ieee.org/document/9341207/}},
    doi = {10.1109/iros45743.2020.9341207},
    keywords = {✅ Read},
    annotation = {00024},
    file = {/home/carlos/Zotero/storage/7QW8STPK/Macenski et al. - 2020 - The marathon 2 A navigation system.pdf}
}

@inproceedings{mayo2021,
    title = {Visual Navigation with Spatial Attention},
    booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
    author = {Mayo, Bar and Hazan, Tamir and Tal, Ayellet},
    year = {2021},
    month = jun,
    pages = {16898--16907},
    keywords = {�� Skimmed},
    annotation = {00004},
    file = {/home/carlos/Zotero/storage/UAEJXTFR/Mayo et al. - 2021 - Visual navigation with spatial attention.pdf}
}

@article{mishra2018,
    title = {A {{Simple Neural Attentive Meta-Learner}}},
    author = {Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
    year = {2018},
    month = feb,
    journal = {arXiv:1707.03141 [cs, stat]},
    eprint = {1707.03141},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    abstract = {Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.},
    archiveprefix = {arXiv},
    keywords = {�� Skimmed},
    file = {/home/carlos/Zotero/storage/AFLUGKRS/Mishra et al. - 2018 - A Simple Neural Attentive Meta-Learner.pdf;/home/carlos/Zotero/storage/ZT57BY8F/1707.html}
}

@article{mitchell2021,
    title = {Offline {{Meta-Reinforcement Learning}} with {{Advantage Weighting}}},
    author = {Mitchell, Eric and Rafailov, Rafael and Peng, Xue Bin and Levine, Sergey and Finn, Chelsea},
    year = {2021},
    month = jul,
    journal = {arXiv:2008.06043 [cs, stat]},
    eprint = {2008.06043},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    abstract = {This paper introduces the offline meta-reinforcement learning (offline meta-RL) problem setting and proposes an algorithm that performs well in this setting. Offline meta-RL is analogous to the widely successful supervised learning strategy of pre-training a model on a large batch of fixed, pre-collected data (possibly from various tasks) and fine-tuning the model to a new task with relatively little data. That is, in offline meta-RL, we meta-train on fixed, pre-collected data from several tasks in order to adapt to a new task with a very small amount (less than 5 trajectories) of data from the new task. By nature of being offline, algorithms for offline meta-RL can utilize the largest possible pool of training data available and eliminate potentially unsafe or costly data collection during meta-training. This setting inherits the challenges of offline RL, but it differs significantly because offline RL does not generally consider a) transfer to new tasks or b) limited data from the test task, both of which we face in offline meta-RL. Targeting the offline meta-RL setting, we propose Meta-Actor Critic with Advantage Weighting (MACAW), an optimization-based meta-learning algorithm that uses simple, supervised regression objectives for both the inner and outer loop of meta-training. On offline variants of common meta-RL benchmarks, we empirically find that this approach enables fully offline meta-reinforcement learning and achieves notable gains over prior methods.},
    archiveprefix = {arXiv},
    keywords = {�� To-Read},
    file = {/home/carlos/Zotero/storage/FIJAKQ3E/Mitchell et al. - 2021 - Offline Meta-Reinforcement Learning with Advantage.pdf;/home/carlos/Zotero/storage/5UUEYKJI/2008.html}
}

@inproceedings{mnih2016,
    title = {Asynchronous Methods for Deep Reinforcement Learning},
    booktitle = {Proceedings of the 33rd International Conference on Machine Learning},
    author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
    editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
    year = {2016},
    month = jun,
    series = {Proceedings of Machine Learning Research},
    volume = {48},
    pages = {1928--1937},
    publisher = {{PMLR}},
    address = {{New York, New York, USA}},
    abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
    pdf = {http://proceedings.mlr.press/v48/mniha16.pdf; https://web.archive.org/web/20211202152319/https://proceedings.mlr.press/v48/mniha16.html},
    keywords = {❌ Unread},
    annotation = {05912},
    file = {/home/carlos/Zotero/storage/95WMLIHW/Mnih2016.pdf}
}

@book{murphy2022,
    title = {Probabilistic {{Machine Learning}}: {{An}} Introduction},
    author = {Murphy, Kevin P.},
    year = {2022},
    publisher = {{MIT Press}},
    annotation = {00041},
    file = {/Users/carlos/Documents/Trabajo/Doctorado/Bibliografia/Libros/Probabilistic Machine Learning_ An Introduction.pdf}
}

@INPROCEEDINGS{jestel2021,
  author={Jestel, Christian and Surmann, Harmtmut and Stenzel, Jonas and Urbann, Oliver and Brehler, Marius},
  booktitle={ICARA},
  title={Obtaining Robust Control and Navigation Policies for Multi-robot Navigation via Deep Reinforcement Learning},
  year={2021},
  volume={},
  number={},
  pages={48-54},
  doi={10.1109/ICARA51699.2021.9376457}}

@inproceedings{ng1999,
    title = {Policy Invariance under Reward Transformations: {{Theory}} and Application to Reward Shaping},
    shorttitle = {Policy Invariance under Reward Transformations},
    booktitle = {ICLR},
    author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart},
    year = {1999},
    pages = {278--287},
    abstract = {This paper investigates conditions under which modifications to the reward function of a Markov decision process preserve the optimal policy. It is shown that, besides the positive linear transformation familiar from utility theory, one can add a reward for transitions between states that is expressible as the difference in value of an arbitrary potential function applied to those states. Furthermore, this is shown to be a necessary condition for invariance, in the sense that any other transformation may yield suboptimal policies unless further assumptions are made about the underlying MDP. These results shed light on the practice of reward shaping, a method used in reinforcement learning whereby additional training rewards are used to guide the learning agent. In particular, some well-known "bugs" in reward shaping procedures are shown to arise from non-potential-based rewards, and methods are given for constructing shaping potentials corresponding to distance-based and subgoalbased h...},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/3GP9PVR8/Ng et al. - 1999 - Policy invariance under reward transformations Th.pdf;/home/carlos/Zotero/storage/TKRAF92P/summary.html}
}

@article{nichol2018,
    title = {On {{First-Order Meta-Learning Algorithms}}},
    author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
    year = {2018},
    month = oct,
    journal = {arXiv:1803.02999 [cs]},
    eprint = {1803.02999},
    eprinttype = {arxiv},
    primaryclass = {cs},
    abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
    archiveprefix = {arXiv},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/UU4F8DYU/Nichol et al. - 2018 - On First-Order Meta-Learning Algorithms.pdf;/home/carlos/Zotero/storage/SFRI4BJ2/1803.html}
}

@inproceedings{park2019,
    title = {Meta-{{Curvature}}},
    booktitle = {Advances in {{Neural Information Processing Systems}}},
    author = {Park, Eunbyung and Oliva, Junier B},
    editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d'{\aftergroup\ignorespaces} {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
    year = {2019},
    volume = {32},
    publisher = {{Curran Associates, Inc.}},
    keywords = {❌ Unread},
    annotation = {00057},
    file = {/home/carlos/Zotero/storage/WQ979T7H/Park y Oliva - 2019 - Meta-Curvature.pdf}
}

@inproceedings{pathak2017,
  author    = {Deepak Pathak and
               Pulkit Agrawal and
               Alexei A. Efros and
               Trevor Darrell},

  title     = {Curiosity-driven Exploration by Self-supervised Prediction},
  booktitle = {ICML},
  volume    = {70},
  pages     = {2778--2787},

  year      = {2017},
  timestamp = {Wed, 29 May 2019 08:41:45 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/PathakAED17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{quigley2015,
    title = {Programming {{Robots}} with {{ROS}}},
    author = {Quigley, Morgan and Gerkey, Brian and Smart, William D.},
    year = {2015},
    month = dec,
    publisher = {{O'Reilly Media}},
    isbn = {978-1-4493-2389-9},
    annotation = {00270},
    file = {/Users/carlos/Documents/Trabajo/Doctorado/Bibliografia/Libros/Programming Robots with ROS.pdf}
}

@inproceedings{rakelly2019,
    title = {Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables},
    booktitle = {Proceedings of the 36th International Conference on Machine Learning},
    author = {Rakelly, Kate and Zhou, Aurick and Finn, Chelsea and Levine, Sergey and Quillen, Deirdre},
    editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
    year = {2019},
    month = jun,
    series = {Proceedings of Machine Learning Research},
    volume = {97},
    pages = {5331--5340},
    publisher = {{PMLR}},
    doi = {https://proceedings.mlr.press/v97/rakelly19a.html},
    abstract = {Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While meta-reinforcement learning (meta-RL) algorithms can enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness on sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.},
    pdf = {http://proceedings.mlr.press/v97/rakelly19a/rakelly19a.pdf},
    keywords = {�� Skimmed},
    annotation = {00257},
    file = {/home/carlos/Zotero/storage/QKIL5Y6R/Rakelly et al. - 2019 - Efficient off-policy meta-reinforcement learning v.pdf}
}

@article{ramakrishnan2021,
    title = {Habitat-{{Matterport 3D Dataset}} ({{HM3D}}): 1000 {{Large-scale 3D Environments}} for {{Embodied AI}}},
    shorttitle = {Habitat-{{Matterport 3D Dataset}} ({{HM3D}})},
    author = {Ramakrishnan, Santhosh K. and Gokaslan, Aaron and Wijmans, Erik and Maksymets, Oleksandr and Clegg, Alexander and Turner, John and Undersander, Eric and Galuba, Wojciech and Westbury, Andrew and Chang, A. and Savva, M. and Zhao, Yili and Batra, Dhruv},
    year = {2021},
    journal = {NeurIPS},
    doi = {https://openreview.net/forum?id=-v4OuqNs5P},
    abstract = {Habitat-Matterport 3D is a large-scale dataset of 1,000 building-scale 3D reconstructions from a diverse set of real-world locations that is `pareto optimal' in the following sense \textendash{} agents trained to perform PointGoal navigation on HM3D achieve the highest performance regardless of whether they are evaluated onHM3D, Gibson, or MP3D. We present the Habitat-Matterport 3D (HM3D) dataset. HM3D is a large-scale dataset of 1,000 building-scale 3D reconstructions from a diverse set of real-world locations. Each scene in the dataset consists of a textured 3D mesh reconstruction of interiors such as multi-floor residences, stores, and other private indoor spaces. HM3D surpasses existing datasets available for academic research in terms of physical scale, completeness of the reconstruction, and visual fidelity. HM3D contains 112.5k m of navigable space, which is 1.4 3.7\texttimes{} larger than other building-scale datasets such as MP3D and Gibson. When compared to existing photorealistic 3D datasets such as Replica, MP3D, Gibson, and ScanNet, images rendered from HM3D have 20 85\% higher visual fidelity w.r.t. counterpart images captured with real cameras, and HM3D meshes have 34 91\% fewer artifacts due to incomplete surface reconstruction. The increased scale, fidelity, and diversity of HM3D directly impacts the performance of embodied AI agents trained using it. In fact, we find that HM3D is `pareto optimal' in the following sense \textendash{} agents trained to perform PointGoal navigation on HM3D achieve the highest performance regardless of whether they are evaluated on HM3D, Gibson, or MP3D. No similar claim can be made about training on Preprint. Under review. ar X iv :2 10 9. 08 23 8v 1 [ cs .C V ] 1 6 Se p 20 21 other datasets. HM3D-trained PointNav agents achieve 100\% performance on Gibson-test dataset, suggesting that it might be time to retire that episode dataset.},
    langid = {english},
    keywords = {❌ Unread},
    annotation = {00000; https://web.archive.org/web/20211202152744/https://www.semanticscholar.org/paper/Habitat-Matterport-3D-Dataset-\%28HM3D\%29:-1000-3D-for-Ramakrishnan-Gokaslan/f46bce5b7dd78736133c1af1824ddb83c0ec2e55},
    file = {/home/carlos/Zotero/storage/CU9GLQH4/Ramakrishnan2021.pdf;/Users/carlos/Documents/Trabajo/Doctorado/Presentaciones/Seminarios/presentation-habitat-matterport.odp}
}

@inproceedings{ramrakhya2022,
    title = {Habitat-{{Web}}: {{Learning Embodied Object-Search Strategies}} from {{Human Demonstrations}} at {{Scale}}},
    shorttitle = {Habitat-{{Web}}},
    booktitle = {CVPR},
    author = {Ramrakhya, Ram and Undersander, Eric and Batra, Dhruv and Das, Abhishek},
    year = {2022},
    abstract = {We present a large-scale study of imitating human demonstrations on tasks that require a virtual robot to search for objects in new environments \textendash{} (1) ObjectGoal Navigation (e.g. `find \& go to a chair') and (2) PICK\&PLACE (e.g. `find mug, pick mug, find counter, place mug on counter'). First, we develop a virtual teleoperation data-collection infrastructure \textendash{} connecting Habitat simulator running in a web browser to Amazon Mechanical Turk, allowing remote users to teleoperate virtual robots, safely and at scale. We collect 80k demonstrations for OBJECTNAV and 12k demonstrations for PICK\&PLACE, which is an order of magnitude larger than existing human demonstration datasets in simulation or on real robots. Our virtual teleoperation data contains 29.3M actions, and is equivalent to 22.6k hours of real-world teleoperation time, and illustrates rich, diverse strategies for solving the tasks. Second, we use this data to answer the question \textendash{} how does large-scale imitation learning (IL) (which has not been hitherto possible) compare to reinforcement learning (RL) (which is the status quo)? On OBJECTNAV, we find that IL (with no bells or whistles) using 70k human demonstrations outperforms RL using 240k agent-gathered trajectories. This effectively establishes an `exchange rate' \textendash{} a single human demonstration appears to be worth {$\sim$}4 agent-gathered ones. More importantly, we find the IL-trained agent learns efficient object-search behavior from humans \textendash{} it peeks into rooms, checks corners for small objects, turns in place to get a panoramic view \textendash{} none of these are exhibited as prominently by the RL agent, and to induce these behaviors via contemporary RL techniques would require tedious reward engineering. Finally, accuracy vs. training data size plots show promising scaling behavior, suggesting that simply collecting more demonstrations is likely to advance the state of art further. On PICK\&PLACE, the comparison is starker \textendash{} IL agents achieve {$\sim$}18\% success on episodes with new object-receptacle locations when trained with 9.5k human demonstrations, while RL agents fail to get beyond 0\%. Overall, our work provides compelling evidence for investing in large-scale imitation learning.},
    file = {/home/carlos/Zotero/storage/FP4DZL3C/Ramrakhya et al. - 2022 - Habitat-Web Learning Embodied Object-Search Strat.pdf}
}

@misc{schulman2016,
    title = {Gradient {{Estimation Using Stochastic Computation Graphs}}},
    author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
    year = {2016},
    month = jan,
    number = {arXiv:1506.05254},
    eprint = {1506.05254},
    eprinttype = {arxiv},
    primaryclass = {cs},
    publisher = {{arXiv}},
    abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.},
    archiveprefix = {arXiv},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/UP7IFYTQ/Schulman et al. - 2016 - Gradient Estimation Using Stochastic Computation G.pdf;/home/carlos/Zotero/storage/TIKTW4EK/1506.html}
}

@article{schulman2017,
    title = {Proximal {{Policy Optimization Algorithms}}},
    author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
    year = {2017},
    month = aug,
    journal = {arXiv:1707.06347 [cs]},
    eprint = {1707.06347},
    eprinttype = {arxiv},
    primaryclass = {cs},
    abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
    archiveprefix = {arXiv},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/7ND47HJ6/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/home/carlos/Zotero/storage/5LW4JNLE/1707.html}
}

@article{schulman2017c,
    title = {Trust {{Region Policy Optimization}}},
    author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
    year = {2017},
    month = apr,
    journal = {arXiv:1502.05477 [cs]},
    eprint = {1502.05477},
    eprinttype = {arxiv},
    primaryclass = {cs},
    abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
    archiveprefix = {arXiv},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/D2AT7GJ6/Schulman et al. - 2017 - Trust Region Policy Optimization.pdf;/home/carlos/Zotero/storage/AHXHDP85/1502.html}
}

@book{sutton2018,
    title = {Reinforcement Learning: An Introduction},
    shorttitle = {Reinforcement Learning},
    author = {Sutton, Richard S. and Barto, Andrew G.},
    year = {2018},
    series = {Adaptive Computation and Machine Learning Series},
    edition = {Second edition},
    publisher = {{The MIT Press}},
    address = {{Cambridge, Massachusetts}},
    abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
    isbn = {978-0-262-03924-6},
    langid = {english},
    lccn = {Q325.6 .R45 2018},
    annotation = {47978},
    file = {/home/carlos/Zotero/storage/CT97AK5A/RLbook2020.pdf}
}

@inproceedings{szot2021,
    title = {Habitat 2.0: {{Training Home Assistants}} to {{Rearrange}} Their {{Habitat}}},
    booktitle = {NeurIPS},
    author = {Szot, Andrew and Clegg, Alex and Undersander, Eric and Wijmans, Erik and Zhao, Yili and Turner, John and Maestre, Noah and Mukadam, Mustafa and Chaplot, Devendra and Maksymets, Oleksandr and Gokaslan, Aaron and Vondrus, Vladimir and Dharur, Sameer and Meier, Franziska and Galuba, Wojciech and Chang, Angel and Kira, Zsolt and Koltun, Vladlen and Malik, Jitendra and Savva, Manolis and Batra, Dhruv},
    year = {2021},
    keywords = {❌ Unread},
    file = {/home/carlos/Zotero/storage/CYIRMZP2/Szot et al. - 2021 - Habitat 2.0 Training Home Assistants to Rearrange.pdf}
}

@inproceedings{tripathi2021,
    title = {Fast Few-Shot Classification by Few-Iteration Meta-Learning},
    booktitle = {2021 {{IEEE}} International Conference on Robotics and Automation ({{ICRA}})},
    author = {Tripathi, Ardhendu Shekhar and Danelljan, Martin and Van Gool, Luc and Timofte, Radu},
    year = {2021},
    pages = {9522--9528},
    doi = {10.1109/ICRA48506.2021.9561269},
    keywords = {�� Skimmed},
    annotation = {00000},
    file = {/home/carlos/Zotero/storage/ES2VESY4/Tripathi et al. - 2021 - Fast few-shot classification by few-iteration meta.pdf}
}

@article{wang2017,
    title = {Learning to Reinforcement Learn},
    author = {Wang, Jane X. and {Kurth-Nelson}, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
    year = {2017},
    month = jan,
    journal = {arXiv:1611.05763 [cs, stat]},
    eprint = {1611.05763},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
    archiveprefix = {arXiv},
    keywords = {�� Skimmed},
    file = {/home/carlos/Zotero/storage/PLLPCTSL/Wang et al. - 2017 - Learning to reinforcement learn.pdf;/home/carlos/Zotero/storage/AD86UM26/1611.html}
}

@inproceedings{wijmans2020,
    title = {{{DD-PPO}}: {{Learning Near-Perfect PointGoal Navigators}} from 2.5 {{Billion Frames}}},
    booktitle = {ICLR},
    author = {Wijmans, Erik and Kadian, Abhishek and Morcos, Ari and Lee, Stefan and Essa, Irfan and Parikh, Devi and Savva, Manolis and Batra, Dhruv},
    year = {2020},
    keywords = {✅ Read},
    annotation = {00094; https://web.archive.org/web/20211001230814/https://openreview.net/forum?id=H1gX8C4YPr},
    file = {/home/carlos/Zotero/storage/INBBAQEV/Wijmans2020.pdf;/Users/carlos/Documents/Trabajo/Doctorado/Presentaciones/Seminarios/Wijmans2020.key}
}

@article{wortsman2019,
    title = {Learning to {{Learn How}} to {{Learn}}: {{Self-Adaptive Visual Navigation Using Meta-Learning}}},
    author = {Wortsman, Mitchell and Ehsani, Kiana and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, R.},
    year = {2019},
    journal = {CVPR},
    pages = {6743--6752},
    doi = {10.1109/cvpr.2019.00691},
    keywords = {✅ Read},
    annotation = {79 citations (Semantic Scholar/DOI) [2021-11-19]},
    file = {/home/carlos/Zotero/storage/DE44GCYZ/Wortsman2019.pdf;/Users/carlos/Documents/Trabajo/Doctorado/Presentaciones/Seminarios/Wortsman2019.pptx}
}

@article{xia2018,
    title = {Gibson {{Env}}: {{Real-World Perception}} for {{Embodied Agents}}},
    shorttitle = {Gibson {{Env}}},
    author = {Xia, Fei and Zamir, Amir and He, Zhi-Yang and Sax, Alexander and Malik, Jitendra and Savarese, Silvio},
    year = {2018},
    month = aug,
    journal = {arXiv:1808.10654 [cs]},
    eprint = {1808.10654},
    eprinttype = {arxiv},
    primaryclass = {cs},
    abstract = {Developing visual perception models for active agents and sensorimotor control are cumbersome to be done in the physical world, as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we are concerned with the problem of developing real-world perception for active agents, propose Gibson Virtual Environment for this purpose, and showcase sample perceptual tasks learned therein. Gibson is based on virtualizing real spaces, rather than using artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism, "Goggles", enabling deploying the trained models in real-world without needing further domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.},
    archiveprefix = {arXiv},
    keywords = {✅ Read},
    file = {/home/carlos/Zotero/storage/IIFWRN8C/Xia et al. - 2018 - Gibson Env Real-World Perception for Embodied Age.pdf;/home/carlos/Zotero/storage/NLF3RS4W/1808.html}
}

@article{xue2020,
    title = {Model-{{Agnostic Metalearning-Based Text-Driven Visual Navigation Model}} for {{Unfamiliar Tasks}}},
    author = {Xue, Tianfang and Yu, Haibin},
    year = {2020},
    journal = {IEEE Access},
    volume = {8},
    pages = {166742--166752},
    issn = {2169-3536},
    doi = {10.1109/ACCESS.2020.3023014},
    abstract = {As vision and language processing techniques have made great progress, mapless-visual navigation is occupying uppermost position in domestic robot field. However, most current end-to-end navigation models tend to be strictly trained and tested on identical datasets with stationary structure, which leads to great performance degradation when dealing with unseen targets and environments. Since the targets of same category could possess quite diverse features, generalization ability of these models is also limited by their visualized task description. In this article we propose a model-agnostic metalearning based text-driven visual navigation model to achieve generalization to untrained tasks. Based on meta-reinforcement learning approach, the agent is capable of accumulating navigation experience from existing targets and environments. When applied to finding a new object or exploring in a new scene, the agent will quickly learn how to fulfill this unfamiliar task through relatively few recursive trials. To improve learning efficiency and accuracy, we introduce fully convolutional instance-aware semantic segmentation and Word2vec into our DRL network to respectively extract visual and semantic features according to object class, creating more direct and concise linkage between targets and their surroundings. Several experiments have been conducted on realistic dataset Matterport3D to evaluate its target-driven navigation performance and generalization ability. The results demonstrate that our adaptive navigation model could navigate to text-defined targets and achieve fast adaption to untrained tasks, outperforming other state-of-the-art navigation approaches.},
    keywords = {✅ Read},
    file = {/home/carlos/Zotero/storage/P4C7D3FD/Xue and Yu - 2020 - Model-Agnostic Metalearning-Based Text-Driven Visu.pdf;/home/carlos/Zotero/storage/2RNWSLPG/9189802.html}
}

@article{xue2021,
    title = {Unbiased {{Model-Agnostic Metalearning Algorithm}} for {{Learning Target-Driven Visual Navigation Policy}}},
    author = {Xue, Tianfang and Yu, Haibin},
    year = {2021},
    month = dec,
    journal = {Computational Intelligence and Neuroscience},
    volume = {2021},
    pages = {e5620751},
    publisher = {{Hindawi}},
    issn = {1687-5265},
    doi = {10.1155/2021/5620751},
    abstract = {As deep reinforcement learning methods have made great progress in the visual navigation field, metalearning-based algorithms are gaining more attention since they greatly improve the expansibility of moving agents. According to metatraining mechanism, typically an initial model is trained as a metalearner by existing navigation tasks and becomes well performed in new scenes through relatively few recursive trials. However, if a metalearner is overtrained on the former tasks, it may hardly achieve generalization on navigating in unfamiliar environments as the initial model turns out to be quite biased towards former ambient configuration. In order to train an impartial navigation model and enhance its generalization capability, we propose an Unbiased Model-Agnostic Metalearning (UMAML) algorithm towards target-driven visual navigation. Inspired by entropy-based methods, maximizing the uncertainty over output labels in classification tasks, we adopt inequality measures used in Economics as a concise metric to calculate the loss deviation across unfamiliar tasks. With succinctly minimizing the inequality of task losses, an unbiased navigation model without overperforming in particular scene types can be learnt based on Model-Agnostic Metalearning mechanism. The exploring agent complies with a more balanced update rule, able to gather navigation experience from training environments. Several experiments have been conducted, and results demonstrate that our approach outperforms other state-of-the-art metalearning navigation methods in generalization ability.},
    langid = {english},
    keywords = {✅ Read},
    file = {/home/carlos/Zotero/storage/K379P8GA/Xue y Yu - 2021 - Unbiased Model-Agnostic Metalearning Algorithm for.pdf;/home/carlos/Zotero/storage/TKQ8CAST/5620751.html}
}

@inproceedings{yan2020,
    title = {Multimodal Aggregation Approach for Memory Vision-Voice Indoor Navigation with Meta-Learning},
    booktitle = {2020 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems ({{IROS}})},
    author = {Yan, Liqi and Liu, Dongfang and Song, Yaoxian and Yu, Changbin},
    year = {2020},
    pages = {5847--5854},
    doi = {10.1109/IROS45743.2020.9341398},
    keywords = {�� Skimmed},
    annotation = {00003},
    file = {/home/carlos/Zotero/storage/7KKLBTAD/Yan et al. - 2020 - Multimodal aggregation approach for memory vision-.pdf}
}

@article{yao2021,
    title = {Meta-Learning with Fewer Tasks through Task Interpolation},
    author = {Yao, Huaxiu and Zhang, Linjun and Finn, Chelsea},
    year = {2021},
    journal = {CoRR},
    volume = {abs/2106.02695},
    eprint = {2106.02695},
    eprinttype = {arxiv},
    doi = {https://arxiv.org/abs/2106.02695},
    archiveprefix = {arXiv},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    biburl = {https://dblp.org/rec/journals/corr/abs-2106-02695.bib},
    keywords = {❌ Unread},
    annotation = {00000},
    timestamp = {Thu, 10 Jun 2021 16:34:18 +0200; https://web.archive.org/web/20210608050928/https://arxiv.org/abs/2106.02695},
    file = {/home/carlos/Zotero/storage/8822IM2P/Yao et al. - 2021 - Meta-learning with fewer tasks through task interp.pdf}
}

@inproceedings{ye2021,
    title = {Auxiliary Tasks and Exploration Enable {{ObjectGoal}} Navigation},
    booktitle = {ICCV},
    author = {Ye, Joel and Batra, Dhruv and Das, Abhishek and Wijmans, Erik},
    year = {2021},
    keywords = {✅ Read},
    file = {/home/carlos/Zotero/storage/XSD9JLGC/Ye et al. - 2021 - Auxiliary tasks and exploration enable ObjectGoal .pdf}
}

@inproceedings{zha2020b,
    title = {Rank the {{Episodes}}: {{A Simple Approach}} for {{Exploration}} in {{Procedurally-Generated Environments}}},
    shorttitle = {Rank the {{Episodes}}},
    booktitle = {ICLR},
    author = {Zha, Daochen and Ma, Wenye and Yuan, Lei and Hu, Xia and Liu, Ji},
    year = {2020},
    month = sep,
    abstract = {Exploration under sparse reward is a long-standing challenge of model-free reinforcement learning. The state-of-the-art methods address this challenge by introducing intrinsic rewards to encourage...},
    langid = {english},
    file = {/home/carlos/Zotero/storage/W8TZW3FM/Zha et al. - 2020 - Rank the Episodes A Simple Approach for Explorati.pdf;/home/carlos/Zotero/storage/R97NID36/Zha et al. - 2020 - Rank the Episodes A Simple Approach for Explorati.html}
}

@article{zha2021,
    title = {{{RANK THE EPISODES}}: {{A SIMPLE APPROACH FOR EXPLORATION IN PROCEDURALLY-GENERATED ENVIRONMENTS}}},
    author = {Zha, Daochen and Ma, Wenye and Yuan, Lei and Hu, Xia and Liu, Ji},
    year = {2021},
    pages = {25},
    abstract = {Exploration under sparse reward is a long-standing challenge of model-free reinforcement learning. The state-of-the-art methods address this challenge by introducing intrinsic rewards to encourage exploration in novel states or uncertain environment dynamics. Unfortunately, methods based on intrinsic rewards often fall short in procedurally-generated environments, where a different environment is generated in each episode so that the agent is not likely to visit the same state more than once. Motivated by how humans distinguish good exploration behaviors by looking into the entire episode, we introduce RAPID, a simple yet effective episode-level exploration method for procedurally-generated environments. RAPID regards each episode as a whole and gives an episodic exploration score from both per-episode and long-term views. Those highly scored episodes are treated as good exploration behaviors and are stored in a small ranking buffer. The agent then imitates the episodes in the buffer to reproduce the past good exploration behaviors. We demonstrate our method on several procedurally-generated MiniGrid environments, a first-person-view 3D Maze navigation task from MiniWorld, and several sparse MuJoCo tasks. The results show that RAPID significantly outperforms the state-of-the-art intrinsic reward strategies in terms of sample efficiency and final performance. The code is available at https://github.com/daochenzha/rapid.},
    langid = {english}
}

@inproceedings{zhang2021,
    title = {Relational Navigation Learning in Continuous Action Space among Crowds},
    booktitle = {2021 {{IEEE}} International Conference on Robotics and Automation ({{ICRA}})},
    author = {Zhang, Xueyou and Xi, Wei and Guo, Xian and Fang, Yongchun and Wang, Bin and Liu, Wulong and Hao, Jianye},
    year = {2021},
    pages = {3175--3181},
    doi = {10.1109/ICRA48506.2021.9561884},
    keywords = {❌ Unread},
    annotation = {00000},
    file = {/home/carlos/Zotero/storage/YF62ZGYS/Zhang et al. - 2021 - Relational navigation learning in continuous actio.pdf}
}

@InProceedings{zhu2017,
  title = {{Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning}},
  author = {Yuke Zhu and Roozbeh Mottaghi and Eric Kolve and Joseph J. Lim and Abhinav Gupta and Li Fei-Fei and Ali Farhadi},
  booktitle = {{ICLR}},
  year = 2017,
}

@inproceedings{zhu2021,
    title = {{{SOON}}: {{Scenario}} Oriented Object Navigation with Graph-Based Exploration},
    booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
    author = {Zhu, Fengda and Liang, Xiwen and Zhu, Yi and Yu, Qizhi and Chang, Xiaojun and Liang, Xiaodan},
    year = {2021},
    month = jun,
    pages = {12689--12699},
    keywords = {�� Skimmed},
    annotation = {00004},
    file = {/home/carlos/Zotero/storage/JHKJZHT3/Zhu et al. - 2021 - SOON Scenario oriented object navigation with gra.pdf}
}

@misc{pyRIL,
    author = {Hernandez-Garcia, Sergio},
    title = {pyRIL: Python Reinforcement and Imitation Learning Library},
    year = {2022},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/SergioHdezG/pyRIL}},
}

@misc{gym_miniworld,
    author = {Chevalier-Boisvert, Maxime},
    title = {MiniWorld: Minimalistic 3D Environment for RL and Robotics Research},
    year = {2018},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/maximecb/gym-miniworld}},
}
@article{wu2020a,
  title={Towards target-driven visual navigation in indoor scenes via generative imitation learning},
  author={Wu, Qiaoyun and Gong, Xiaoxi and Xu, Kai and Manocha, Dinesh and Dong, Jingxuan and Wang, Jun},
  journal={IEEE Robotics and Automation Letters},
  volume={6},
  number={1},
  pages={175--182},
  year={2020},
  publisher={IEEE}
}

@inproceedings{newcombe2011,
  title = {{{KinectFusion}}: {{Real-time}} Dense Surface Mapping and Tracking},
  shorttitle = {{{KinectFusion}}},
  booktitle = {{{International Symposium}} on {{Mixed}} and {{Augmented Reality}}},
  author = {Newcombe, Richard A. and Izadi, Shahram and Hilliges, Otmar and Molyneaux, David and Kim, David and Davison, Andrew J. and Kohi, Pushmeet and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
  year = {2011},
  abstract = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
  keywords = {AR,Cameras,Dense Reconstruction,Depth Cameras,GPU,Image reconstruction,Iterative closest point algorithm,Real time systems,Real-Time,Simultaneous localization and mapping,SLAM,Surface reconstruction,Three dimensional displays,Tracking,Volumetric Representation},
  file = {/home/carlos/Zotero/storage/65437UVV/Newcombe et al. - 2011 - KinectFusion Real-time dense surface mapping and .pdf;/home/carlos/Zotero/storage/QX3VQSEJ/6162880.html}
}


@article{jones2011,
author = {Eagle S. Jones and Stefano Soatto},
title ={Visual-inertial navigation, mapping and localization: A scalable real-time causal approach},
journal = {The International Journal of Robotics Research},
volume = {30},
number = {4},
pages = {407-430},
year = {2011},
doi = {10.1177/0278364910388963},
URL = { 
        https://doi.org/10.1177/0278364910388963
},
eprint = { 
        https://doi.org/10.1177/0278364910388963
}
,
    abstract = { We describe a model to estimate motion from monocular visual and inertial measurements. We analyze the model and characterize the conditions under which its state is observable, and its parameters are identifiable. These include the unknown gravity vector, and the unknown transformation between the camera coordinate frame and the inertial unit. We show that it is possible to estimate both state and parameters as part of an on-line procedure, but only provided that the motion sequence is ‘rich enough’, a condition that we characterize explicitly. We then describe an efficient implementation of a filter to estimate the state and parameters of this model, including gravity and camera-to-inertial calibration. It runs in real-time on an embedded platform. We report experiments of continuous operation, without failures, re-initialization, or re-calibration, on paths of length up to 30 km. We also describe an integrated approach to ‘loop-closure’, that is the recognition of previously seen locations and the topological re-adjustment of the traveled path. It represents visual features relative to the global orientation reference provided by the gravity vector estimated by the filter, and relative to the scale provided by their known position within the map; these features are organized into ‘locations’ defined by visibility constraints, represented in a topological graph, where loop-closure can be performed without the need to re-compute past trajectories or perform bundle adjustment. The software infrastructure as well as the embedded platform is described in detail in a previous technical report. }
}

@inproceedings{sattler2018,
title = "Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions",
abstract = "Visual localization enables autonomous vehicles to navigate in their surroundings and augmented reality applications to link virtual to real worlds. Practical visual localization approaches need to be robust to a wide variety of viewing condition, including day-night changes, as well as weather and seasonal variations, while providing highly accurate 6 degree-of-freedom (6DOF) camera pose estimates. In this paper, we introduce the first benchmark datasets specifically designed for analyzing the impact of such factors on visual localization. Using carefully created ground truth poses for query images taken under a wide variety of conditions, we evaluate the impact of various factors on 6DOF camera pose estimation accuracy through extensive experiments with state-of-the-art localization approaches. Based on our results, we draw conclusions about the difficulty of different conditions, showing that long-term localization is far from solved, and propose promising avenues for future work, including sequence-based localization approaches and the need for better local features. Our benchmark is available at visuallocalization.net.",
author = "Torsten Sattler and Will Maddern and Carl Toft and Akihiko Torii and Lars Hammarstrand and Erik Stenborg and Daniel Safari and Masatoshi Okutomi and Marc Pollefeys and Josef Sivic and Fredrik Kahl and Tomas Pajdla",
year = "2018",
booktitle = "CVPR",
}

@article{thrun2001,
  title = {Robust {{Monte Carlo}} Localization for Mobile Robots},
  author = {Thrun, Sebastian and Fox, Dieter and Burgard, Wolfram and Dellaert, Frank},
  year = {2001},
  month = may,
  journal = {Artificial Intelligence},
  volume = {128},
  number = {1},
  pages = {99--141},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(01)00069-8},
  urldate = {2023-06-14},
  abstract = {Mobile robot localization is the problem of determining a robot's pose from sensor data. This article presents a family of probabilistic localization algorithms known as Monte Carlo Localization (MCL). MCL algorithms represent a robot's belief by a set of weighted hypotheses (samples), which approximate the posterior under a common Bayesian formulation of the localization problem. Building on the basic MCL algorithm, this article develops a more robust algorithm called Mixture-MCL, which integrates two complimentary ways of generating samples in the estimation. To apply this algorithm to mobile robots equipped with range finders, a kernel density tree is learned that permits fast sampling. Systematic empirical results illustrate the robustness and computational efficiency of the approach.},
  langid = {english},
  keywords = {Kernel density trees,Localization,Mobile robots,Particle filters,Position estimation},
  file = {/home/carlos/Zotero/storage/ERWZSN6U/Thrun et al. - 2001 - Robust Monte Carlo localization for mobile robots.pdf;/home/carlos/Zotero/storage/LXBSYJDX/S0004370201000698.html}
}

@inproceedings{ramrakhya2023,
  title = {{{PIRLNav}}: {{Pretraining}} with {{Imitation}} and {{RL Finetuning}} for {{ObjectNav}}},
  shorttitle = {{{PIRLNav}}},
  author = {Ramrakhya, Ram and Batra, Dhruv and Wijmans, Erik and Das, Abhishek},
  year = {2023},
  booktitle={CVPR},
}

@InProceedings{zhang2022,
author="Zhang, Sixian
and Li, Weijie
and Song, Xinhang
and Bai, Yubing
and Jiang, Shuqiang",
title="Generative Meta-Adversarial Network for Unseen Object Navigation",
booktitle="ECCV",
year="2022",
abstract="Object navigation is a task to let the agent navigate to a target object. Prevailing works attempt to expand navigation ability in new environments and achieve reasonable performance on the seen object categories that have been observed in training environments. However, this setting is somewhat limited in real world scenario, where navigating to unseen object categories is generally unavoidable. In this paper, we focus on the problem of navigating to unseen objects in new environments only based on limited training knowledge. Same as the common ObjectNav tasks, our agent still gets the egocentric observation and target object category as the input and does not require any extra inputs. Our solution is to let the agent ``imagine`` the unseen object by synthesizing features of the target object. We propose a generative meta-adversarial network (GMAN), which is mainly composed of a feature generator and an environmental meta discriminator, aiming to generate features for unseen objects and new environments in two steps. The former generates the initial features of the unseen objects based on the semantic embedding of the object category. The latter enables the generator to further learn the background characteristics of the new environment, progressively adapting the generated features to approximate the real features of the target object. The adapted features serve as a more specific representation of the target to guide the agent. Moreover, to fast update the generator with a few observations, the entire adversarial framework is learned in the gradient-based meta-learning manner. The experimental results on AI2THOR and RoboTHOR simulators demonstrate the effectiveness of the proposed method in navigating to unseen object categories. The code is available at https://github.com/sx-zhang/GMAN.git.",
}

@InProceedings{radford2021,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {ICLR},
  pages = 	 {8748--8763},
  year = 	 {2021},
  volume = 	 {139},
  month = 	 {18--24 Jul},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@software{minigrid,
    author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
    title = {Minimalistic Gridworld Environment for Gymnasium},
    url = {https://github.com/Farama-Foundation/Minigrid},
    year = {2018},
}

@misc{mnih2013,
    doi = {10.48550/ARXIV.1312.5602},
    url = {https://arxiv.org/abs/1312.5602},
    author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
    keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Playing Atari with Deep Reinforcement Learning},
    publisher = {arXiv},
    year = {2013},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{campos2021,
  author={Campos, Carlos and Elvira, Richard and Rodríguez, Juan J. Gómez and M. Montiel, José M. and D. Tardós, Juan},
  journal={IEEE Transactions on Robotics}, 
  title={ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual–Inertial, and Multimap SLAM}, 
  year={2021},
}

@inproceedings{sumikura2019,
author = {Sumikura, Shinya and Shibuya, Mikiya and Sakurada, Ken},
title = {OpenVSLAM: A Versatile Visual SLAM Framework},
year = {2019},
abstract = {In this paper, we introduce OpenVSLAM, a visual SLAM framework with high usability and extensibility. Visual SLAM systems are essential for AR devices, autonomous control of robots and drones, etc. However, conventional open-source visual SLAM frameworks are not appropriately designed as libraries called from third-party programs. To overcome this situation, we have developed a novel visual SLAM framework. This software is designed to be easily used and extended. It incorporates several useful features and functions for research and development. OpenVSLAM is released at https://github.com/xdspacelab/openvslam under the 2-clause BSD license.},
booktitle = {International Conference on Multimedia},
}

@ARTICLE{labbe2022,
  
AUTHOR={Labbé, Mathieu and Michaud, François},   
	 
TITLE={Multi-Session Visual SLAM for Illumination-Invariant Re-Localization in Indoor Environments},      
	
JOURNAL={Frontiers in Robotics and AI},      

YEAR={2022},
ABSTRACT={For robots navigating using only a camera, illumination changes in indoor environments can cause re-localization failures during autonomous navigation. In this paper, we present a multi-session visual SLAM approach to create a map made of multiple variations of the same locations in different illumination conditions. The multi-session map can then be used at any hour of the day for improved re-localization capability. The approach presented is independent of the visual features used, and this is demonstrated by comparing re-localization performance between multi-session maps created using the RTAB-Map library with SURF, SIFT, BRIEF, BRISK, KAZE, DAISY, and SuperPoint visual features. The approach is tested on six mapping and six localization sessions recorded at 30 min intervals during sunset using a Google Tango phone in a real apartment.}
}

@article{ma2017,
  title={Multi-view deep learning for consistent semantic mapping with RGB-D cameras},
  author={Lingni Ma and J. St{\"u}ckler and Christian Kerl and Daniel Cremers},
  journal={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2017},
  pages={598-605}
}

@article{zhang2018,
  title={Semantic SLAM Based on Object Detection and Improved Octomap},
  author={Liang Zhang and Le-Yi Wei and Peiyi Shen and Wei Wei and Guangming Zhu and Juan Song},
  journal={IEEE Access},
  year={2018},
  volume={6},
  pages={75545-75559}
}

@article{rosinol2020,
  title={Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping},
  author={Antoni Rosinol and Marcus Abate and Yun Chang and Luca Carlone},
  journal={ICRA},
  year={2020},
  pages={1689-1696}
}

@inproceedings{yadav2022,
      title={Offline Visual Representation Learning for Embodied Navigation}, 
      author={Karmesh Yadav and Ram Ramrakhya and Arjun Majumdar and Vincent-Pierre Berges and Sachit Kuhar and Dhruv Batra and Alexei Baevski and Oleksandr Maksymets},
      year={2023},
      booktitle={ICLR},
}

@inproceedings{bansal2019,
  title = {{{ChauffeurNet}}: {{Learning}} to {{Drive}} by {{Imitating}} the {{Best}} and {{Synthesizing}} the {{Worst}}},
  shorttitle = {{{ChauffeurNet}}},
  booktitle = {Robotics: {{Science}} and {{Systems XV}}},
  author = {Bansal, Mayank and Krizhevsky, Alex and Ogale, Abhijit},
  year = {2019},
  month = jun,
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2019.XV.031},
  urldate = {2023-06-14},
  abstract = {Our goal is to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. We find that standard behavior cloning is insufficient for handling complex driving scenarios, even when we leverage a perception system for preprocessing the input and a controller for executing the output on the car: 30 million examples are still not enough. We propose exposing the learner to synthesized data in the form of perturbations to the expert's driving, which creates interesting situations such as collisions and/or going off the road. Rather than purely imitating all data, we augment the imitation loss with additional losses that penalize undesirable events and encourage progress \textendash{} the perturbations then provide an important signal for these losses and lead to robustness of the learned model. We show that the ChauffeurNet model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of our proposed changes and show that the model is responding to the appropriate causal factors. Finally, we demonstrate the model driving a real car at our test facility.},
  isbn = {978-0-9923747-5-4},
  langid = {english},
  file = {/home/carlos/Zotero/storage/KSBGEP23/Bansal et al. - 2019 - ChauffeurNet Learning to Drive by Imitating the B.pdf}
}

@inproceedings{
	peng2020,
	author = {Peng, Xue Bin and Coumans, Erwin and Zhang, Tingnan and Lee, Tsang-Wei Edward and Tan, Jie and Levine, Sergey},
	booktitle={Robotics: Science and Systems},
	year = {2020},
	month = {07},
	title = {Learning Agile Robotic Locomotion Skills by Imitating Animals},
	doi = {10.15607/RSS.2020.XVI.064}
}
@inproceedings{
baker2022,
title={Video PreTraining ({VPT}): Learning to Act by Watching Unlabeled Online Videos},
author={Bowen Baker and Ilge Akkaya and Peter Zhokov and Joost Huizinga and Jie Tang and Adrien Ecoffet and Brandon Houghton and Raul Sampedro and Jeff Clune},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=AXDNM76T1nc}
}

@article{Zhang2017DeepIL,
  title={Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation},
  author={Tianhao Zhang and Zoe McCarthy and Owen Jow and Dennis Lee and Ken Goldberg and P. Abbeel},
  journal={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2017},
  pages={1-8}
}

@article{Tanwani2020Motion2VecSR,
  title={Motion2Vec: Semi-Supervised Representation Learning from Surgical Videos},
  author={Ajay Kumar Tanwani and Pierre Sermanet and Andy Yan and Raghav V. Anand and Mariano Phielipp and Ken Goldberg},
  journal={2020 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2020},
  pages={2174-2181}
}

@article{peters2008,
author = {Peters, Jan and Schaal, Stefan},
title = {Reinforcement Learning of Motor Skills with Policy Gradients},
year = {2008},
abstract = {Autonomous learning is one of the hallmarks of human and animal behavior, and understanding the principles of learning will be crucial in order to achieve true autonomy in advanced machines like humanoid robots. In this paper, we examine learning of complex motor skills with human-like limbs. While supervised learning can offer useful tools for bootstrapping behavior, e.g., by learning from demonstration, it is only reinforcement learning that offers a general approach to the final trial-and-error improvement that is needed by each individual acquiring a skill. Neither neurobiological nor machine learning studies have, so far, offered compelling results on how reinforcement learning can be scaled to the high-dimensional continuous state and action spaces of humans or humanoids. Here, we combine two recent research developments on learning motor control in order to achieve this scaling. First, we interpret the idea of modular motor control by means of motor primitives as a suitable way to generate parameterized control policies for reinforcement learning. Second, we combine motor primitives with the theory of stochastic policy gradient learning, which currently seems to be the only feasible framework for reinforcement learning for humanoids. We evaluate different policy gradient methods with a focus on their applicability to parameterized motor primitives. We compare these algorithms in the context of motor primitive learning, and show that our most modern algorithm, the Episodic Natural Actor-Critic outperforms previous algorithms by at least an order of magnitude. We demonstrate the efficiency of this reinforcement learning method in the application of learning to hit a baseball with an anthropomorphic robot arm.},
journal = {Neural Netw.},
keywords = {Natural Actor-Critic, Reinforcement learning, Policy gradient methods, Motor skills, Motor primitives, Natural gradients}
}

@inproceedings{schaal1996,
 author = {Schaal, Stefan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M.C. Mozer and M. Jordan and T. Petsche},
 pages = {},
 publisher = {MIT Press},
 title = {Learning from Demonstration},
 url = {https://proceedings.neurips.cc/paper_files/paper/1996/file/68d13cf26c4b4f4f932e3eff990093ba-Paper.pdf},
 volume = {9},
 year = {1996}
}

@inproceedings{gutierrez2019,
  title={Collision Anticipation via Deep Reinforcement Learning for Visual Navigation},
  author={Eduardo Guti{\'e}rrez-Maestro and Roberto Javier L{\'o}pez-Sastre and Saturnino Maldonado-Basc{\'o}n},
  booktitle={IbPRIA},
  year={2019}
}

@article{yang2018,
  title={Visual Semantic Navigation using Scene Priors},
  author={Wei Yang and X. Wang and Ali Farhadi and Abhinav Kumar Gupta and Roozbeh Mottaghi},
  journal={ICLR},
  year={2018},
}

@article{Mousavian2018,
  title={Visual Representations for Semantic Target Driven Navigation},
  author={Arsalan Mousavian and Alexander Toshev and Marek Fiser and Jana Kosecka and James Davidson},
  journal={ICRA},
  year={2018},
}

@inproceedings{
shah2022,
title={Offline Reinforcement Learning for Visual Navigation},
author={Dhruv Shah and Arjun Bhorkar and Hrishit Leen and Ilya Kostrikov and Nicholas Rhinehart and Sergey Levine},
booktitle={CoRL},
year={2022},
}

@article{Zhou2023,
  title={{ESC}: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation},
  author={KAI-QING Zhou and Kai Zheng and Connor Pryor and Yilin Shen and Hongxia Jin and Lise Getoor and Xin Eric Wang},
  journal={ArXiv},
  year={2023},
}

@article{Huang2023,
  title={Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control},
  author={Wenlong Huang and F. Xia and Dhruv Shah and Danny Driess and Andy Zeng and Yao Lu and Peter R. Florence and Igor Mordatch and Sergey Levine and Karol Hausman and Brian Ichter},
  journal={ArXiv},
  year={2023}
}

@article{LOLA,
  title={Assistive Robot with an AI-Based Application for the Reinforcement of Activities of Daily Living: Technical Validation with Users Affected by Neurodevelopmental Disorders},
  author={Nadia Nasri and Roberto Javier L{\'o}pez-Sastre and Soraya Pacheco-da-Costa and Iv{\'a}n Fern{\'a}ndez-Munilla and Carlos Guti{\'e}rrez-{\'A}lvarez and Thais Pousada-Garc{\'i}a and Francisco Javier Acevedo-Rodr{\'i}guez and Saturnino Maldonado-Basc{\'o}n},
  journal={Applied Sciences},
  year={2022}
}

@INPROCEEDINGS{mask-rcnn,
  author={He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
  booktitle={ICCV},
  title={Mask {R-CNN}}, 
  year={2017},
}

@inproceedings{Son2020,
author = {Son, Dongwon and Yang, Hyunsoo and Lee, Dongjun},
title = {Sim-to-Real Transfer of Bolting Tasks with Tight Tolerance},
year = {2020},
publisher = {IEEE Press},
abstract = {In this paper, we propose a novel sim-to-real framework to solve bolting tasks with tight tolerance and complex contact geometry which are hard to be modeled. The sim-to-real has desirable features in terms of cost and safety, however, that of the assembly task is rare due to the lack of simulator, which can robustly render multi-contact assembly. We implement the sim-to-real transfer of nut tightening policy which is adaptive to uncertain bolt positions. This can be realized through developing a novel contact model, which is fast and robust to complex assembly geometry, and novel hierarchical controller with reinforcement learning (RL), which can perform the tasks with a narrow and complicated path. The fast and robust contact model is achieved by utilizing configuration space abstraction and passive midpoint integrator (PMI), which render the simulator robust even in a high stiffness contact condition. And we use sampling-based motion planning to construct a path library and design linear quadratic tracking controller as a low-level controller to be compliant and avoid local optima. Additionally, we use the RL agent as a high-level controller to make it possible to adapt to the bolt position uncertainty, thereby realizing sim-to-real. Experiments are performed to verify our proposed sim-to-real framework.},
booktitle = {IROS},
pages = {9056–9063},
numpages = {8},
location = {Las Vegas, NV, USA}
}

@inproceedings{sadeghiCAD2RLRealSingleImage2017,
  title = {{{CAD2RL}}: {{Real Single-Image Flight Without}} a {{Single Real Image}}},
  shorttitle = {{{CAD2RL}}},
  booktitle = {Robotics: {{Science}} and {{Systems}}},
  author = {Sadeghi, Fereshteh and Levine, Sergey},
  year = {2017},
  file = {C:\Users\accou\Zotero\storage\872HKN3S\Sadeghi y Levine - 2017 - CAD2RL Real Single-Image Flight Without a Single .pdf}
}

@article{Hwangbo_2019,
	year = 2019,
	author = {Jemin Hwangbo and Joonho Lee and Alexey Dosovitskiy and Dario Bellicoso and Vassilios Tsounis and Vladlen Koltun and Marco Hutter},
	title = {Learning agile and dynamic motor skills for legged robots},
	journal = {Science Robotics}
}

@inproceedings{agarwal2022,
  title = {Legged {{Locomotion}} in {{Challenging Terrains}} Using {{Egocentric Vision}}},
  booktitle = {CoRL},
  author = {Agarwal, Ananye and Kumar, Ashish and Malik, Jitendra and Pathak, Deepak},
  year = {2022},

  file = {C\:\\Users\\accou\\Zotero\\storage\\L8JJNDWR\\Agarwal et al. - 2022 - Legged Locomotion in Challenging Terrains using Eg.pdf;C\:\\Users\\accou\\Zotero\\storage\\3KXQ4VFP\\forum.html}
}

@article{gervet2022,
  title = {Navigating to {{Objects}} in the {{Real World}}},
  author = {Gervet, Theophile and Chintala, Soumith and Batra, Dhruv and Malik, Jitendra and Chaplot, Devendra Singh},
  year = {2022},
  journal = {Science Robotics},
}

@ARTICLE{Liu2022,
  author={Liu, Xinzhu and Guo, Di and Liu, Huaping and Sun, Fuchun},
  journal={IEEE Robotics and Automation Letters},
  title={Multi-Agent Embodied Visual Semantic Navigation With Scene Prior Knowledge},
  year={2022},
  volume={7},
  number={2},
  pages={3154-3161},
  doi={10.1109/LRA.2022.3145964}}

@ARTICLE{Wang2023,
  author={Wang, Hongcheng and Wang, Yuxuan and Zhong, Fangwei and Wu, Mingdong and Zhang, Jianwei and Wang, Yizhou and Dong, Hao},
  journal={IEEE Robotics and Automation Letters},
  title={Learning Semantic-Agnostic and Spatial-Aware Representation for Generalizable Visual-Audio Navigation},
  year={2023},
}

@article{Kondoh2023MultigoalAN,
  title={Multi-goal Audio-visual Navigation using Sound Direction Map},
  author={Haruo Kondoh and Asako Kanezaki},
  journal={ArXiv},
  year={2023},
}

@article{Yadav2023OVRLV2AS,
  title={OVRL-V2: A simple state-of-art baseline for ImageNav and ObjectNav},
  author={Karmesh Yadav and Arjun Majumdar and Ram Ramrakhya and Naoki Yokoyama and Alexei Baevski and Zsolt Kira and Oleksandr Maksymets and Dhruv Batra},
  journal={ArXiv},
  year={2023}, }

